---
title       : Miles Per Gallon (MPG) Predictor
subtitle    : predict MPG for vehicles from 1974
author      : szelenka
job         : 
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : []            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---
```{r echo=F}
library(car)
library(ggplot2)
library(grid)
fit = lm(mpg~cyl+wt+disp,mtcars)
model = "lm(mpg~cyl+wt+disp,mtcars)"
```
## Executive Summary

- Can you trust the Miles Per Gallon (MPG) advertised on a used vehicle from 1974?
- How much has fuel effeciency changed since 1974 for a vehicle with the same specifications?

Using this ShinnyApp, it's ridiculously easy to answer these questions! This web based application runs natively in your web browser, without the need to install anything. 

This ShinnyApp is able to predict with user defined degrees of confidece, with only 3 data points about the vehicle in question. All you need to do is enter in the desired `Cylinders`, `Displacement`, `Weight`, and `Confidence Interval` of the vehicle from 1974 you would like to predict Miles Per Gallon (MPG) on.

The prediction function fits a linear model using the `mtcars` dataset which was extracted from the 1974 Motor Trend US magazine, for 32 automobiles (1973â€“74 models). Using the formula `mpg ~ cy+wt+disp`.

https://szelenka.shinyapps.io/devdataprod-project/

--- .class #id 

## Analysis of Variance

```{r}
summary(aov(lm(mpg~.,mtcars)))
```

---

## Model Selection

```{r}
fit = lm(mpg~cyl+wt+disp,mtcars)
```
- From the Analysis of Variance output in the previous slide, we can see Cylinder (`cyl`), Displacement (`disp`), and Weight (`wt`) have a value below 5% and are considred influential towards the response variable Miles Per Gallon (`mpg`).
- The proportion of the variation explained by these selected features is `r summary(fit)$r.squared`.
- `r summary(fit)$r.squared*100`% considered `strong positively related`!
- Residuals plot (next slide) implies a good fit of the data, because the number and degree of positive residuals is offset by an apparent equal number and degree of negative residuals.
- This implies our training data is pretty good to model unknown data points against, and the prediction model used in this ShinyApp.

---

## Graphs

```{r fig.width=13, fig.height=8, echo=F, message=F}
fortify.fit = fortify(fit)
fortify.fit = cbind(fortify.fit,rows=1:nrow(fortify.fit))
g1 = ggplot(fortify.fit, aes(.fitted, .resid)) +
  geom_point()  +
  geom_smooth(se=F, method=lm) +
  geom_hline(linetype=2, size=.2) +
  labs(
    x=paste("Fitted Values\n",model,sep=""),
    y="Residual",
    title="Residuals vs Fitted"
  )
a = quantile(fit$residuals, c(0.25, 0.75))
b = qnorm(c(0.25, 0.75))
slope = diff(a)/diff(b)
int = a[1] - slope * b[1]
g2 = ggplot(fortify.fit, aes(sample=.resid)) +
  stat_qq() +
  geom_abline(slope=slope, intercept=int, colour="blue") +
  labs(
    x=paste("Theoretical Quantiles\n",model,sep=""),
    y="Standardized Residuals",
    title="Normal QQ"
  )
g3 = ggplot(fortify.fit, aes(.fitted, sqrt(abs(.stdresid)))) +
  geom_point() +
  geom_smooth(se=F, method=loess) +
  geom_abline(slope=slope, intercept=int) +
  labs(
    x=paste("Fitted Values\n",model,sep=""),
    y="Sqrt of Standardized Residuals",
    title="Scale-Location"
  )
g4 = ggplot(fortify.fit, aes(rows, .cooksd, ymin=0, ymax=.cooksd)) +
  geom_point() +
  geom_linerange() +
  scale_x_continuous(breaks=seq(0,max(fortify.fit$rows),5)) +
  labs(
    x=paste("Obs. number\n",model,sep=""),
    y="Cook's Distance",
    title="Cook's Distance"
  )
g5 = ggplot(fortify.fit, aes(.hat, .stdresid)) +
  geom_point() +
  geom_smooth(se=F, method=loess) +
  geom_hline(linetype=2, size=.2) +
  labs(
    x=paste("Leverage\n",model,sep=""),
    y="Standardized residuals",
    title="Residuals vs Leverage"
  )
g6 = ggplot(fortify.fit, aes(.hat, .cooksd)) +
  geom_point() +
  geom_smooth(se=F, method=loess) +
  labs(
    x=paste("Leverage\n",model,sep=""),
    y="Cook's Distance",
    title="Cook's Distance vs Leverage"
  )
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  require(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
multiplot(g1,g3,g5,g2,g4,g6,cols=3)
```
