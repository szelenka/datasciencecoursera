membership(fc)
plot(fc)
modularity(fc)
sizes(fc)
sum(sizes(fc))
sizes = sizes(fc)
sapply(sizes, function(x){ x/27475})
sapply(sizes, function(x){ x/27475*100})
V(g)$label[membership(fc)==30]
sapply(sizes, function(x){ x/27475*100})[1:4]
sum(sapply(sizes, function(x){ x/27475*100})[1:4])
sum(sapply(sizes, function(x){ x/27475*100}))
sizes(imc)
V(g)$label[membership(fc)==30]
V(g)$label[membership(imc)==30]
sizes(imc)[30]
V(g)$label[membership(imc)==50]
V(g)$label[membership(imc)==150]
V(g)$label[membership(imc)==156]
?modularity
modularity(fc)
modularity(imc)
answer = modularity(fc) - modularity(imc)
answer
graph.coreness(k.core)
g.coreness()
k.core
table(k.core)
answer = length(largest.clique[[1]])
answer
fc.community.sizes = sizes(fc)
# four largest communities comprise rougly what percentage of the graph?
answer = sum(sapply(fc.community.sizes, function(x) {
x/sum(fc.community.sizes)
})[1:4])
answer
fc.community.sizes = sizes(fc)
# four largest communities comprise rougly what percentage of the graph?
answer = sum(sapply(fc.community.sizes, function(x) {
x/sum(fc.community.sizes)
})[1:5])
answer
sum(sapply(fc.community.sizes, function(x) {
+     x/sum(fc.community.sizes)
+ })
sum(sapply(fc.community.sizes, function(x) {
x/sum(fc.community.sizes)
})
)
answer = sapply(fc.community.sizes, function(x) {
x/sum(fc.community.sizes)
})
answer
head(answer)
?sort
head(sort(answer))
head(sort(answer,decreasing=T))
head(sort(answer,decreasing=T))
sum(sort(sapply(fc.community.sizes, function(x) {
x/sum(fc.community.sizes)
}),decreasing=T)[1:4])
.5^5
1/16
.5*5
.5*5/5
sum(dbinom(1:5, 5, .5))
dbinom(5, 5, .5)
dbinom(4, 5, .5)
n=795
n*(n-1)/2
n/852
1-0.9330986
40*sqrt(36)
200^2
1200*.2
.1*40sqrt(400)
.1*40*sqrt(400)
.2*400
.2*401
.1*40*sqrt(401)
(1/6)*(1/3)*(1/4)*(1/4)
1-(1/9)
library(caret)
library(kernlab)
data(spam)
head(spam)
plot(density(spam$your[spam$type=="nonspam"]),
col="blue", main="", xlab="Frequency of 'your'")
lines(density(spam$your[spam$type="spam"]),col="red")
plot(density(spam$your[spam$type=="nonspam"]),
col="blue", main="", xlab="Frequency of 'your'")
lines(density(spam$your[spam$type=="spam"]),col="red")
abline(v=.5,col="black")
prediction = ifelse(spam$your > .5, "spam","nonspam")
table(prediction,spam$type/length(spam$type))
prediction = ifelse(spam$your > .5, "spam","nonspam")
table(prediction,spam$type)/length(spam$type))
prediction = ifelse(spam$your > .5, "spam","nonspam")
table(prediction,spam$type)/length(spam$type)
a = table(prediction,spam$type)/length(spam$type)
a[0]
a[1]
a[4]
set.seed(333)
smallSpam = spam[sample(dim(spam)[1],size=10),]
spamLabel = (smallSpam$type=="spam")*1+1
plot(smallSpam$capitalAve,col-spamLabel)
set.seed(333)
smallSpam = spam[sample(dim(spam)[1],size=10),]
spamLabel = (smallSpam$type=="spam")*1+1
plot(smallSpam$capitalAve,col=spamLabel)
rule1 = function(x) {
prediction = rep(NA, length(x))
prediction[x > 2.7] = "spam"
prediction[x < 2.4] = "nonspam"
prediction[x >= 2.4 & x <= 2.45] = "spam"
prediction[x > 2.45 & x <= 2.7] = "nonspam"
return(prediction)
}
table(rule1(smallSpam$capitalAve),smallSpam$type)
rule2 = function(x) {
prediction = rep(NA, length(x))
prediction[x > 2.8] = "spam"
prediction[x >= 2.8] = "nonspam"
return(prediction)
}
table(rule2(smallSpam$capitalAve),smallSpam$type)
rule2 = function(x) {
prediction = rep(NA, length(x))
prediction[x > 2.8] = "spam"
prediction[x<= 2.8] = "nonspam"
return(prediction)
}
table(rule2(smallSpam$capitalAve),smallSpam$type)
table(rule1(spam$capitalAve,spam$type))
table(rule2(spam$capitalAve,spam$type))
mean(rule1(spam$capitalAve)==spam$type)
table(rule1(spam$capitalAve,spam$type))
table(rule2(spam$capitalAve),spam$type)
mean(rule1(spam$capitalAve)==spam$type)
table(rule1(spam$capitalAve),spam$type)
table(rule2(spam$capitalAve),spam$type)
mean(rule1(spam$capitalAve)==spam$type)
mean(rule2(spam$capitalAve)==spam$type)
sum(rule1(spam$capitalAve)==spam$type)
sum(rule2(spam$capitalAve)==spam$type)
source('~/Documents/_svn/datasciencecoursera/premachlearn/project/weightliftingexercise.R')
source('~/Documents/_svn/datasciencecoursera/premachlearn/project/weightliftingexercise.R')
head(test)
head(train)
nrow(train)
?sample
?rep
?random
dbinom(5)
dbinom(5,1)
dbinom(5,1,.5)
dbinom(5,10,.5)
sample(c(0,1),1)
sample(c(0,1),5)
sample(c(0,1),5,replace=T)
sample(c(0,1),nrow(train_org),replace=T)
train_org = loadData(urlTrainData,"pml-training.csv")
sample(c(0,1),nrow(train_org),replace=T)
source('~/Documents/_svn/datasciencecoursera/premachlearn/project/weightliftingexercise.R')
validate = train_org[index,]
train = train_org[-index,]
sum(index)
sum(index)-nrow(train_org)
train = train_org[-1*index,]
index = sample(c(F,T),nrow(train_org),replace=T)
validate = train_org[index,]
train = train_org[-index,]
head(-index)
head(index)
head(!index)
validate = train_org[index,]
train = train_org[!index,]
nrow(train)+nrow(validate)
nrow(train)+nrow(validate)==nrow(train_org)
plot(validate)
str(train)
source('~/Documents/_svn/datasciencecoursera/premachlearn/project/weightliftingexercise.R')
library(caret)
library(kernlab)
data(spam)
inTrain = createDataPartition(y=spam$type, p=.75, list=F)
training = spam[inTrain]
testing = spam[-inTrain]
dim(training)
source('~/Documents/_svn/datasciencecoursera/premachlearn/week2.R')
source('~/Documents/_svn/datasciencecoursera/premachlearn/week2.R')
modelFit
modelFit$finalModel
predictions = predict(modelFit,newdata=testing)
preditcions
predictions = predict(modelFit,newdata=testing)
predictions
confusionMatrix(predictions,testing$type)
source('~/Documents/_svn/datasciencecoursera/premachlearn/week2.R')
set.seed(32323)
folds = createFolds(y=spam$type, k=10, list=T, returnTrain=T)
sapply(folds,length)
folds[[1]][1:10]
folds = createResampling(y=spam$type, times=10, list=T)
sapply(folds,length)
folds[[1]][1:10]
set.seed(32323)
folds = createResample(y=spam$type, times=10, list=T)
sapply(folds,length)
folds[[1]][1:10]
set.seed(32323)
tme = 1:1000
folds = createTimeSlices(y=tme, initialWindow=20, horizon=10)
sapply(folds,length)
folds[[1]][1:10]
names(folds)
folds$train[[1]]
folds$test[[1]]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
qplot(seq_along(training$CompressiveStrength), training$CompressiveStrength, color=training$FlyAsh)
library(ggplot2)
qplot(seq_along(training$CompressiveStrength), training$CompressiveStrength, color=training$FlyAsh)
library(ISLR)
library(ggplot2)
library(caret)
data(Wage)
summary(Wage)
install.packages(ISLR)
install.packages("ISLR")
library(ISLR)
library(ggplot2)
library(caret)
data(Wage)
summary(Wage)
?train
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
qplot(seq_along(training$CompressiveStrength), training$CompressiveStrength, color=training$FlyAsh)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
qplot(seq_along(training$CompressiveStrength), training$CompressiveStrength, color=training$FlyAsh)
seq_along(training$CompressiveStrength)
qplot(seq_along(training$CompressiveStrength), training$CompressiveStrength, color=training$FlyAsh)
training$FlyAsh
library(ggplot2)
qplot(seq_along(training$CompressiveStrength), training$CompressiveStrength, color=training$FlyAsh)
?qplot
20^2
2^20
require(devtools)
devtools::install_github('rstudio/shinyapps')
shinyapps::setAccountInfo(name='szelenka',
token='ECB7079D96A83E168D7C6FA3379AA74F',
secret='<SECRET>')
shinyapps::setAccountInfo(name='szelenka',
token='ECB7079D96A83E168D7C6FA3379AA74F',
secret='rJZONzEF81QryjRR/baVouGEjf2QfryVSWGD6nnm')
require(devtools)
install_github('ropensci/plotly')
library(plotly)
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot, s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), slider = x(0, 2, step = 0.1))
manipulate(myPlot(s), x.s = slider(0, 2, step = 0.1))
library(airquality)
require(rCharts)
install.packages("airquality")
airquality
dTable(airquality, sPaginationType = "full_numbers")
?h2
library(shiny)
?h2
library(devtools)
install_github('slidify')
install_github('slidify','')
install_github('ramnathv/slidify','')
install_github('ramnthv/slidifyLibraries','')
install_github('ramnathv/slidifyLibraries','')
library(slidify)
getwd()
setwd('/Users/szelenka/Documents/_svn/datasciencecoursera/devdataprod')
author('first_deck')
slidify('first_deck/index.Rmd')
slidify('index.Rmd')
library(knitr)
browseURL('index.html')
getwd()
getMethod()
getS3method()
showMethods()
getMethod("show")
show
getMethod("show")
getClass("show")
getS3method("show")
showMethod("show")
showMethods("show")
?base::predict
??base::summary
??base::lm
??base::colSums
??base::predict
??base::dgamma
??base::dgamma
??base::predict
??base::colSums
??base::lm
??base::colSums
??base::predict
??predict
?predict
?base::predict
?base:predict
?mtcars
max(mtcars$disp)
min(mtcars$disp)
levels(mtcars$cyl)
mtcars$cyl
getwd()
setwd("/Users/szelenka/Documents/_svn/datasciencecoursera/devdataprod/project")
runApp()
library(shiny)
runApp()
runApp()
runApp()
library(car)
library(ggplot2)
library(grid)
data(mtcars)
mtcars$am = factor(mtcars$am)
levels(mtcars$am) = c("automatic","manual")
fit = lm(mpg~cyl+wt+disp,mtcars)
predict(fit,data.frame(cyl='4',disp=70,wt=2))
predict(fit,data.frame(cyl=4,disp=70,wt=2))
runApp()
runApp()
as.numeric('4')
runApp()
predict(fit,data.frame(cyl=4,disp=70,wt=2))
predict(fit,data.frame(cyl=4,disp=70,wt=2))[0]
predict(fit,data.frame(cyl=4,disp=70,wt=2))[1]
predict(fit,data.frame(cyl=4,disp=70,wt=2))[[1]]
(predict(fit,data.frame(cyl=4,disp=70,wt=2))[[1]])
runApp()
t.test(mgp~cyl+disp+wt,data=mtcars,paried=F)
t.test(mpg~cyl+disp+wt,data=mtcars,paried=F)
t.test(mpg~cyl+disp+wt,data=mtcars,paried=FALSE)
t.test(mpg~am,data=mtcars,paried=FALSE)
t.test(mpg~cyl,data=mtcars,paried=FALSE)
t.test(mpg~,data=mtcars,paried=FALSE)
mtcars$cyl
levels(mtcats$cyl)
levels(mtcats$cyl) = c('4','6','8')
levels(mtcars$cyl) = c('4','6','8')
mtcars$cyl
data(mtcars)
mtcars$cyl
as.factor(mtcars$cyl)
t.text(mpg~cyl,data=mtcars,paried=F)
t.test(mpg~cyl,data=mtcars,paried=F)
t.test(mpg~disp,data=mtcars,paried=F)
t.test(mpg~wt,data=mtcars,paried=F)
mtcars$mpg
?t.test
formula = mpg~cyl+wt+disp
formula
nrows(mtcars)
nrow(mtcars)
qnorm(0.975)*s/sqrt(n)
plot(mtcars)
plot(mtcars$mpg)
sd(mtcars$sd)
?sd
sd(mtcars$sd,na.rm=T)
sd(mtcars$mpg)
a = 5
s = sd(mtcars$mpg)
n = nrow(mtcars)
error = qnorm(0.975)*s/sqrt(n)
error
left
mean(mtcars$mpg
)
runApp()
summary(fit)
summary(fit)$p.value
names(summary(fit))
summary(fit)$fstatistic
summary(fit)$
s
fit
names(fit)
coef(fit)
summary(fit)$fstatistic
f = summary(fit)$fstatistic
pf(f[1],f[2],f[3])
pf(f[1],f[2],f[3],lower.tail=F)
runApp()
runApp()
runApp()
a = c(8,9,10,11,12)
mean(a)
var(a)
sd(a)
a
sum((a-10)^2)
a
(a-10^2)
(a-10)^2
sqrt(10)
mean(a)
var(a)
?var
sqrt(car(a))
sqrt(var(a))
a
sqrt(10)
sum((a-mean(a))^2)
1/sum((a-mean(a))^2)
sqrt(1/sum((a-mean(a))^2))
sum((a-mean(a))^2)/10
sqrt(1)
mean(a)
a-mean(a)
(a-mean(a))^2
sum((a-mean(a))^2)
sum((a-mean(a))^2)/5
sqrt(2)
qt(.05,29)
qt(.975,29)
1/10000
1/10000*.99
var(a)
sum((a-mean(a))^2)/5
sum((a-mean(a))^2)/length(a)
sum((a-mean(a))^2)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
parse(text='2^2')
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp(display.mode="showcase")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
